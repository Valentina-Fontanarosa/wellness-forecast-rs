{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUM33CSWK8fU"
   },
   "source": [
    "# Caricamento librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8533,
     "status": "ok",
     "timestamp": 1744451647922,
     "user": {
      "displayName": "Valentina Fontanarosa",
      "userId": "03790539537871065652"
     },
     "user_tz": -120
    },
    "id": "kMAKFweuLDFL",
    "outputId": "d426e58c-4f2a-4b90-9517-67b3a883a39b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.11/dist-packages (3.6.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Collecting utilsforecast\n",
      "  Downloading utilsforecast-0.2.12-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh) (3.1.6)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh) (6.0.2)\n",
      "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh) (6.4.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh) (2025.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading utilsforecast-0.2.12-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: utilsforecast\n",
      "Successfully installed utilsforecast-0.2.12\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scipy matplotlib seaborn bokeh statsmodels scikit-learn utilsforecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744451647928,
     "user": {
      "displayName": "Valentina Fontanarosa",
      "userId": "03790539537871065652"
     },
     "user_tz": -120
    },
    "id": "9I1iyweCLahR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4553,
     "status": "ok",
     "timestamp": 1744451652482,
     "user": {
      "displayName": "Valentina Fontanarosa",
      "userId": "03790539537871065652"
     },
     "user_tz": -120
    },
    "id": "EDWGkPD9LeWv"
   },
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Handling and processing of Data\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Handling and processing of Data for Date (time)\n",
    "# ==============================================================================\n",
    "import datetime\n",
    "\n",
    "# Plot\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling with Sklearn\n",
    "# ==============================================================================\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define the plot size\n",
    "# ==============================================================================\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (18,7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efawVa9mjSnl"
   },
   "source": [
    "# **Caso di studio: previsione del benessere giornaliero di un utente**\n",
    "\n",
    "L’obiettivo di questo progetto è valutare se, attraverso modelli di deep learning per il forecasting, sia possibile prevedere il livello di benessere giornaliero di un utente, rappresentato da un indice sintetico chiamato ***activity index*** , utilizzando le informazioni relative alla durata del sonno e all’andamento dell’attività fisica, come la velocità del passo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZzB8T9jMPqG"
   },
   "source": [
    "# **Dataset originale**\n",
    "\n",
    "Il dataset originale è composto da 4 colonne:\n",
    "\n",
    "* `user_id`: identificatore dell'utente\n",
    "* `date`: giorno in cui è stato raccolto il dato aggregato\n",
    "* `data_type`: intero che identifica la tipologia di dato raccolto\n",
    "* `data_value`: valore associato al dato raccolto\n",
    "\n",
    "I diversi valori della variabile `data_type` si riferiscono a dati aggregati raccolti giornalmente dal dispositivo di ogni singolo utente e identificano le tipologie di dato in base al seguente schema:\n",
    "\n",
    "1.  numero totale di passi effettuati\n",
    "2.  peso (kg)\n",
    "3.  BMI (kg/m^2)\n",
    "4.  pressione sanguigna sistolica (mmHg)\n",
    "5.  velocità dell'onda sfigmica arteriosa (PWV), (m/s)\n",
    "6.  PWV healthiness (1: bassa, 2: sano, 3: troppo alta)\n",
    "7.  frequenza cardiaca media (bpm)\n",
    "8.  frequenza cardiaca minima (bpm)\n",
    "9.  frequenza cardiaca massima (bpm)\n",
    "10. durata del sonno (ore)\n",
    "11. orario in cui l'utente si è messo a letto\n",
    "12. orario in cui l'utente si è alzato dal letto\n",
    "13. numero di volte in cui l'utente si è svegliato durante il sonno\n",
    "14. durata del tempo in cui l'utente si è svegliato durante il sonno (ore)\n",
    "15. tempo impiegato dall'utente per addormentarsi (ore)\n",
    "16. tempo impiegato dall'utente per alzarsi dal letto (ore)\n",
    "17. durata di sonno leggero (ore)\n",
    "18. durata di sonno REM (ore)\n",
    "19. durata di sonno profondo (ore)\n",
    "20. tipo di attività\n",
    "21. durata dell'attività (secondi)\n",
    "22. calorie consumate durante l'attività\n",
    "23. frequenza cardiaca media durante l'attività (bpm)\n",
    "24. frequenza cardiaca minima durante l'attività (bpm)\n",
    "25. frequenza cardiaca massima durante l'attività (bpm)\n",
    "26. velocità dell'andatura dei passi (passi al minuto)\n",
    "27. velocità dell'andatura a distanza (km all'ora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzeEEqwZNPjy"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "MYDRIVE = 'drive/MyDrive/Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R84HU_N-MYXE"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(MYDRIVE + \"data/data.csv\", usecols=[1, 2, 3, 4])\n",
    "df.columns = [\"user_id\", \"date_local\", \"data_type\", \"value\"]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZv7-Au5ONsE"
   },
   "outputs": [],
   "source": [
    "df_data_type = pd.read_csv(MYDRIVE + \"data/data_type.csv\")\n",
    "print(df_data_type.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCWKj93jOTv_"
   },
   "source": [
    "# **Creazione dataset per la previsione**\n",
    "\n",
    "E' stato costruito un dataset personalizzato contenente esclusivamente i dati utili all’analisi, con particolare focus sulle variabili legate al sonno e all’attività fisica giornaliera di ciascun utente, in particolare sono state estratte le feature: *sleepduration, bedin, bedout, nbawake, awakeduration, timetosleep, timetowake, remduration, deepduration, activity_type, activity_duration, activity_calories, stepsgaitspeed, distancegaitspeed*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2_-btONOaXd"
   },
   "outputs": [],
   "source": [
    "# Mappatura data_type → nome feature\n",
    "feature_map = {\n",
    "    10: \"sleepduration\", 11: \"bedin\", 12: \"bedout\", 13: \"nbawake\",\n",
    "    14: \"awakeduration\", 15: \"timetosleep\", 16: \"timetowake\", 18: \"remduration\",\n",
    "    19: \"deepduration\", 20: \"activity_type\", 21: \"activity_duration\",\n",
    "    22: \"activity_calories\", 26: \"stepsgaitspeed\", 27: \"distancegaitspeed\"\n",
    "}\n",
    "\n",
    "# Creazione lista di DataFrame per ciascuna feature\n",
    "dataset_list = [\n",
    "    df[df['data_type'] == dtype][['user_id', 'date_local', 'value']]\n",
    "    .rename(columns={'date_local': 'date', 'value': name})\n",
    "    for dtype, name in feature_map.items()\n",
    "]\n",
    "\n",
    "# Merge progressivo su 'user_id' e 'date'\n",
    "from functools import reduce\n",
    "\n",
    "dataset = reduce(lambda left, right: pd.merge(left, right, on=['user_id', 'date'], how='outer'), dataset_list)\n",
    "\n",
    "# Pulizia finale\n",
    "dataset.dropna(subset=['sleepduration', 'stepsgaitspeed'], inplace=True)\n",
    "dataset.set_index(['user_id', 'date'], inplace=True)\n",
    "\n",
    "# Salvataggio\n",
    "dataset.to_csv(MYDRIVE + 'data/processed_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w79nRVPFOzIl"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(MYDRIVE + 'data/processed_dataset.csv', index_col=[0, 1])  # Assicura che user_id e date siano indici\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fjQ6rdLngcZ"
   },
   "source": [
    "# **Calcolo indice di attività fisica**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZeLZdURnNJF"
   },
   "source": [
    "La funzione calcola un punteggio (activity_index) per ogni riga del dataset (cioè per ogni giorno di ogni utente), bilanciando:\n",
    "\n",
    "- Bonus per attività e qualità del sonno, ad esempio chi fa movimento e consuma calorie\n",
    "- Penalizza poco movimento, poco sonno e risvegli notturni\n",
    "\n",
    "Questo indice, ottenuto tramite una funzione appositamente definita, combina diverse informazioni relative all’attività fisica e alla qualità del sonno (ad esempio calorie bruciate, durata del sonno e risvegli notturni) in un unico valore numerico utile per la previsione del benessere giornaliero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6oPCgirpy2Z"
   },
   "outputs": [],
   "source": [
    "def get_activity_score(row):\n",
    "    score = 0\n",
    "\n",
    "    # Calorie bruciate\n",
    "    if not pd.isna(row['activity_calories']):\n",
    "        if row['activity_calories'] > 500:\n",
    "            score += 3\n",
    "        elif row['activity_calories'] > 300:\n",
    "            score += 2\n",
    "        elif row['activity_calories'] > 100:\n",
    "            score += 1\n",
    "\n",
    "    # Durata attività fisica\n",
    "    if not pd.isna(row['activity_duration']):\n",
    "        duration_min = row['activity_duration'] / 60\n",
    "        if duration_min > 60:\n",
    "            score += 3\n",
    "        elif duration_min > 30:\n",
    "            score += 2\n",
    "        elif duration_min > 10:\n",
    "            score += 1\n",
    "\n",
    "    # Sleepduration\n",
    "    if not pd.isna(row['sleepduration']):\n",
    "        if row['sleepduration'] >= 7:\n",
    "            score += 2\n",
    "        elif row['sleepduration'] >= 6:\n",
    "            score += 1\n",
    "\n",
    "    # Awakeduration\n",
    "    if not pd.isna(row['awakeduration']):\n",
    "        if row['awakeduration'] <= 0.5:\n",
    "            score += 2\n",
    "        elif row['awakeduration'] <= 1:\n",
    "            score += 1\n",
    "\n",
    "    # Deep sleep\n",
    "    if not pd.isna(row['deepduration']):\n",
    "        if row['deepduration'] > 1:\n",
    "            score += 1\n",
    "\n",
    "    # REM\n",
    "    if not pd.isna(row['remduration']):\n",
    "        if 1 <= row['remduration'] <= 2:\n",
    "            score += 1\n",
    "\n",
    "    # Passo veloce\n",
    "    if not pd.isna(row['stepsgaitspeed']):\n",
    "        if row['stepsgaitspeed'] > 100:\n",
    "            score += 1\n",
    "\n",
    "    # Distanza\n",
    "    if not pd.isna(row['distancegaitspeed']):\n",
    "        if row['distancegaitspeed'] > 2:\n",
    "            score += 1\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdZumDjmnaO8"
   },
   "source": [
    "La colonna activity_index viene calcolata e aggiunta al dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gv3WExC4dWSI"
   },
   "outputs": [],
   "source": [
    "dataset['activity_index'] = dataset.apply(get_activity_score, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7QYITCSqAEI"
   },
   "source": [
    "# **Creazione sequenze valide di 15 giorni**\n",
    "\n",
    "Per la preparazione dei dati è stato implementato un algoritmo che costruisce sequenze temporali di 15 giorni consecutivi (con massimo un giorno di gap) per ciascun utente. Questo passaggio consente di trasformare i dati grezzi in finestre temporali strutturate, fondamentali per permettere al modello Informer di apprendere le dinamiche temporali e fare previsioni sull'andamento futuro dell'activity index, stimando il valore dell’indice di attività del giorno successivo alla sequenza, ovvero il sedicesimo giorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-WhAi8pqBrO"
   },
   "outputs": [],
   "source": [
    "target_sequence_len = 15\n",
    "sequences = []\n",
    "target = []\n",
    "\n",
    "# Resettiamo e convertiamo 'date' in datetime\n",
    "dataset = dataset.reset_index()\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "dataset = dataset.set_index(['user_id', 'date'])\n",
    "\n",
    "dataset = dataset.sort_index(level=['user_id', 'date'])  # Ordina\n",
    "\n",
    "current_sequence = []\n",
    "last_user = None\n",
    "last_date = None\n",
    "\n",
    "for (user, date), row in dataset.iterrows():\n",
    "    if last_user is not None:\n",
    "        if user == last_user:\n",
    "            if (date - last_date).days in [1, 2]:\n",
    "                current_sequence.append(((user, date), row))\n",
    "                if len(current_sequence) == target_sequence_len:\n",
    "                    sequences.append(current_sequence.copy())\n",
    "                    current_sequence.pop(0)\n",
    "            else:\n",
    "                current_sequence = [((user, date), row)]\n",
    "        else:\n",
    "            current_sequence = [((user, date), row)]\n",
    "    else:\n",
    "        current_sequence.append(((user, date), row))\n",
    "\n",
    "    last_user = user\n",
    "    last_date = date\n",
    "\n",
    "print(f\"Numero totale di sequenze valide trovate: {len(sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylHvlfyvq4zW"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Estrai l'user_id da ogni sequenza\n",
    "user_ids_in_sequences = [sequence[0][0][0] for sequence in sequences]  # sequence[0][0][0] -> user_id della prima riga della sequenza\n",
    "\n",
    "# Conta le sequenze per ogni user_id\n",
    "user_counts = Counter(user_ids_in_sequences)\n",
    "\n",
    "# Stampa il conteggio\n",
    "print(\"Numero sequenze per user_id:\", user_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz2IO3wkIc6Q"
   },
   "source": [
    "# **Preparazione dataset finale per NeuralForecast**\n",
    "\n",
    "In questa fase viene costruito il dataset finale utilizzato per l’addestramento del modello Informer. Per ogni sequenza temporale valida, vengono estratte le principali variabili di interesse: passi, durata del sonno e activity index. Successivamente, per ogni sequenza, viene identificata la data del giorno target, ovvero il giorno successivo alla fine della sequenza, e recuperato il relativo valore di activity_index che rappresenta il valore da prevedere (target y). Solo le sequenze per cui è disponibile il valore target vengono mantenute nel dataset finale.\n",
    "Infine, i dati vengono strutturati secondo il formato richiesto dalla libreria NeuralForecast, rinominando la colonna dell’utente in unique_id e creando un dataframe pronto per l’addestramento del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTciPjNSIeRS"
   },
   "outputs": [],
   "source": [
    "final_data = []\n",
    "\n",
    "for seq in sequences:\n",
    "    user_id = seq[0][0][0]  # user_id della sequenza\n",
    "    seq_sorted = sorted(seq, key=lambda x: x[0][1])  # Ordina per data\n",
    "\n",
    "    # Costruzione del DataFrame della sequenza con anche l'activity_index\n",
    "    data = []\n",
    "    for (user, date), row in seq_sorted:\n",
    "        data.append({\n",
    "            'user_id': user,\n",
    "            'date': date,\n",
    "            'stepsgaitspeed': row['stepsgaitspeed'],\n",
    "            'sleepduration': row['sleepduration'],\n",
    "            'activity_index': row['activity_index']\n",
    "        })\n",
    "    seq_df = pd.DataFrame(data)\n",
    "\n",
    "    # Trova la data del giorno target (giorno dopo la fine della sequenza)\n",
    "    last_date_in_sequence = seq_sorted[-1][0][1]\n",
    "    target_date = last_date_in_sequence + pd.Timedelta(days=1)\n",
    "\n",
    "    # Recupero del target (activity_index del giorno successivo)\n",
    "    try:\n",
    "        target_row = dataset['activity_index'].loc[(user_id, target_date)]\n",
    "        target_value = target_row.iloc[0] if isinstance(target_row, pd.Series) else target_row\n",
    "    except KeyError:\n",
    "        continue  # Salta la sequenza se non esiste il target\n",
    "\n",
    "    if pd.isna(target_value):\n",
    "        continue  # Salta la sequenza se il target è NaN\n",
    "\n",
    "    seq_df['ds'] = seq_df['date']\n",
    "    seq_df['y'] = target_value\n",
    "    final_data.append(seq_df[['user_id', 'ds', 'stepsgaitspeed', 'sleepduration', 'activity_index', 'y']])\n",
    "\n",
    "# Concatenazione finale\n",
    "if final_data:\n",
    "    df_seq = pd.concat(final_data).reset_index(drop=True)\n",
    "    df_seq.rename(columns={'user_id': 'unique_id'}, inplace=True)\n",
    "    print(\"Dataset finale NeuralForecast creato con successo!\")\n",
    "    print(df_seq.head())\n",
    "else:\n",
    "    print(\"Nessuna sequenza completa con target trovata. Dataset vuoto.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyxW9BORd0e7"
   },
   "source": [
    "# **Split Train/Test**\n",
    "\n",
    "Dopo la preparazione del dataset finale, è stato effettuato uno split dei dati in un set di addestramento e un set di test per valutare le performance del modello in fase di previsione. La suddivisione è avvenuta per utente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg77zEqqd15k"
   },
   "outputs": [],
   "source": [
    "# Controllo che df_seq esista\n",
    "if 'df_seq' not in locals():\n",
    "    print(\"Errore: df_seq non è definito. Esegui prima la preparazione del dataset.\")\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Controllo il dataset\n",
    "    print(\"df_seq caricato correttamente!\")\n",
    "    print(df_seq.head())\n",
    "\n",
    "    # Divido gli utenti unici per evitare leakage\n",
    "    user_ids = df_seq['unique_id'].unique()\n",
    "    train_users, test_users = train_test_split(user_ids, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Costruisco i due dataset\n",
    "    train_df = df_seq[df_seq['unique_id'].isin(train_users)].reset_index(drop=True)\n",
    "    test_df = df_seq[df_seq['unique_id'].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "    # Stampo le dimensioni\n",
    "    print(f\"Dimensione train: {train_df.shape}\")\n",
    "    print(f\"Dimensione test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqZE93VGjJrl"
   },
   "outputs": [],
   "source": [
    "!pip install neuralforecast torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKnDAjQnt7yt"
   },
   "source": [
    "# **Definizione del modello Informer utilizzando NeuralForcast**\n",
    "\n",
    "Per la fase di modellazione è stato utilizzato il framework NeuralForecast, che consente di integrare modelli di deep learning per la previsione di serie temporali multivariate. Nello specifico, è stato implementato il modello Informer.\n",
    "I parametri utilizzati sono stati gestiti direttamente dalla libreria NeuralForecast, che si occupa di costruire e addestrare l’intera architettura encoder-decoder del modello in modo automatico e ottimizzato per il forecasting.\n",
    "\n",
    "**Il modello Informer - Breve descrizione e funzionamento dell’architettura**\n",
    "\n",
    "Nel progetto è stato utilizzato Informer, un modello di deep learning basato su architettura Transformer, progettato per la previsione su serie temporali lunghe e multivariate.\n",
    "\n",
    "A differenza dei Transformer classici, l’Informer è stato ottimizzato per gestire lunghe sequenze riducendo il carico computazionale e migliorando la capacità predittiva grazie a:\n",
    "\n",
    "- ProbSparse Attention: seleziona solo le informazioni più rilevanti all’interno della sequenza, riducendo il numero di operazioni necessarie e migliorando l’efficienza sui dati lunghi.\n",
    "- Attention Distillation: riduce progressivamente la complessità della sequenza filtrando i segnali meno importanti e mantenendo solo le componenti dominanti.\n",
    "- Decoder efficiente (MLP): consente di generare più previsioni in un solo passaggio, velocizzando la fase di inferenza.\n",
    "- Embedding temporali e posizionali: catturano sia le dipendenze tra i giorni che la posizione temporale delle osservazioni.\n",
    "\n",
    "**Funzionamento dell’architettura Informer**\n",
    "\n",
    "L’architettura dell’Informer si basa su due componenti principali:\n",
    "\n",
    "- Encoder: riceve la sequenza di input (nel nostro caso 15 giorni di dati su attività e sonno) e applica la ProbSparse Attention e la distillazione per estrarre le dipendenze temporali più rilevanti, riducendo la dimensione della sequenza ma mantenendo l’informazione principale.\n",
    "\n",
    "- Decoder: utilizza l’output dell’encoder e genera la previsione per l’orizzonte desiderato (nel progetto, il giorno 16). Il decoder è progettato per elaborare in modo efficiente anche lunghe finestre temporali e restituire sia la previsione puntuale che le bande di incertezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMNIAsRCeGRY"
   },
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import Informer\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "\n",
    "# Numero di giorni della sequenza passata al modello (15 giorni * 3 variabili)\n",
    "input_size = target_sequence_len * 3  # stepsgaitspeed + sleepduration + activity_index\n",
    "\n",
    "model = Informer(\n",
    "    h=1,  # Previsione a 1 giorno avanti\n",
    "    input_size=input_size,\n",
    "    hidden_size=512,\n",
    "    conv_hidden_size=128,\n",
    "    n_head=8,\n",
    "    loss=DistributionLoss(distribution='StudentT', level=[80, 95]),\n",
    "    scaler_type='standard',\n",
    "    learning_rate=1e-3,\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    max_steps=500,\n",
    "    val_check_steps=50,\n",
    "    windows_batch_size=32,\n",
    "    batch_size=32,\n",
    "    # Ottimizzazioni\n",
    "    dropout=0.2,                   # Aiuta contro l'overfitting\n",
    "    factor=4,                      # Aumenta la sparsity\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkOugZFvLxWS"
   },
   "outputs": [],
   "source": [
    "# Istanzio NeuralForecast\n",
    "nf = NeuralForecast(models=[model], freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt-p24MUl29j"
   },
   "source": [
    "# **Addestramento del modello**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyX-Ezg_l4W9"
   },
   "outputs": [],
   "source": [
    "print(\"Inizio addestramento Informer...\")\n",
    "nf.fit(df=train_df, val_size=5)\n",
    "print(\"Addestramento completato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQdHtmH4mNQU"
   },
   "source": [
    "# **Previsioni sul test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WV1p3YWVRGF"
   },
   "source": [
    "Modelli:\n",
    "\n",
    "- Informer ➔ la previsione puntuale del modello\n",
    "- Informer-hi-80 / hi-95 ➔ parte alta dell'intervallo di predizione (80% - 95%)\n",
    "- Informer-lo-80 / lo-95 ➔ parte bassa dell'intervallo di predizione\n",
    "- Informer-median ➔ la mediana della distribuzione predetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OEKuZ9PmQ2O"
   },
   "outputs": [],
   "source": [
    "forecast = nf.predict(df=test_df)\n",
    "print(\"Previsione completata!\")\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5_gtgyM0-o3"
   },
   "outputs": [],
   "source": [
    "print(f\"Forecast shape: {forecast.shape}\")\n",
    "print(forecast.head())\n",
    "\n",
    "# Assicuro i tipi corretti per il merge\n",
    "forecast['unique_id'] = forecast['unique_id'].astype(str)\n",
    "forecast['ds'] = pd.to_datetime(forecast['ds'])\n",
    "dataset = dataset.reset_index()\n",
    "dataset['user_id'] = dataset['user_id'].astype(str)\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "dataset = dataset.set_index(['user_id', 'date'])\n",
    "\n",
    "# Creazione della lista dei target reali basandomi sulle date previste\n",
    "true_targets = []\n",
    "for idx, row in forecast.iterrows():\n",
    "    try:\n",
    "        y_real = dataset.loc[(row['unique_id'], row['ds']), 'activity_index']\n",
    "        # Se ci sono più valori, prendere il primo\n",
    "        y_real = y_real.iloc[0] if isinstance(y_real, pd.Series) else y_real\n",
    "        true_targets.append({'unique_id': row['unique_id'], 'ds': row['ds'], 'y_real': y_real})\n",
    "    except KeyError:\n",
    "        # Se non esiste la coppia, mettere NaN\n",
    "        true_targets.append({'unique_id': row['unique_id'], 'ds': row['ds'], 'y_real': np.nan})\n",
    "\n",
    "# Costruzione del DataFrame dei target reali\n",
    "real_targets_df = pd.DataFrame(true_targets)\n",
    "print(\"\\nTarget recuperati:\")\n",
    "print(real_targets_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhoRAsXH3Ao-"
   },
   "source": [
    "# **Valutazione Classica sul test**\n",
    "\n",
    "In questa fase è stata effettuata la valutazione quantitativa del modello. Le previsioni generate dal modello Informer sono state unite ai valori reali dell’activity index recuperati dal dataset tramite un'operazione di merge sulla chiave composta da utente (unique_id) e data (ds).\n",
    "Prima del calcolo delle metriche, sono stati eliminati eventuali record con valori mancanti per garantire la correttezza delle stime. La valutazione è stata condotta solo sulle righe con dati completi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uFZ-i4yxv6J"
   },
   "outputs": [],
   "source": [
    "# Merge tra forecast e target reali\n",
    "merged = pd.merge(forecast, real_targets_df, on=['unique_id', 'ds'], how='inner')\n",
    "\n",
    "# Rimozione di eventuali NaN prima del calcolo\n",
    "merged = merged.dropna(subset=['y_real', 'Informer'])\n",
    "print(f\"\\nRighe valide per il calcolo: {merged.shape[0]}\")\n",
    "\n",
    "# Calcolo delle metriche solo se ci sono righe valide\n",
    "if merged.empty:\n",
    "    print(\"Nessuna riga valida per calcolare le metriche.\")\n",
    "else:\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "    mse = mean_squared_error(merged['y_real'], merged['Informer'])\n",
    "    mae = mean_absolute_error(merged['y_real'], merged['Informer'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(merged['y_real'], merged['Informer'])\n",
    "\n",
    "    print(\"\\nRISULTATI MODELLO INFORMER\")\n",
    "    print(f\"MSE:  {mse:.3f}\")\n",
    "    print(f\"MAE:  {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"R²:   {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sywf58kfTFnp"
   },
   "source": [
    "# **Valutazione del modello - CrossValidation**\n",
    "\n",
    "Oltre alla valutazione delle prestazioni su test set, è stata eseguita una cross-validation temporale per simulare il comportamento del modello durante l’addestramento e valutare la sua stabilità nel tempo.\n",
    "\n",
    "La cross-validation è stata effettuata con 10 finestre (n_windows=10), ciascuna rappresentante uno scenario di addestramento e validazione su porzioni differenti della serie temporale. Per ogni finestra è stato calcolato l’errore assoluto medio (MAE) come metrica di riferimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJecmhGFTItd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae, mean_squared_error\n",
    "\n",
    "#Eseguo la cross-validation\n",
    "cv_result = nf.cross_validation(\n",
    "    df=train_df,\n",
    "    n_windows=10  # Numero di split da effettuare\n",
    ")\n",
    "\n",
    "print(\"Cross-validation completata!\")\n",
    "print(cv_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8uO82y-zpjU"
   },
   "outputs": [],
   "source": [
    "# Calcolo MAE per ogni finestra della cross-validation\n",
    "epoch_mae = cv_result.groupby('cutoff').apply(\n",
    "    lambda g: mae(g['y'], g['Informer'])\n",
    ").reset_index(name='mae_loss')\n",
    "\n",
    "# Applicazione dello smoothing alla MAE\n",
    "epoch_mae['mae_smooth'] = epoch_mae['mae_loss'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot MAE smussata\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epoch_mae['mae_smooth'], color='blue', label='Smoothed Validation MAE')\n",
    "plt.fill_between(epoch_mae.index, epoch_mae['mae_smooth'], alpha=0.2, color='blue')\n",
    "\n",
    "# Trovo il minimo\n",
    "min_idx = epoch_mae['mae_smooth'].idxmin()\n",
    "min_val = epoch_mae['mae_smooth'].min()\n",
    "plt.scatter(min_idx, min_val, color='red', label=f'Min MAE: {min_val:.2f}')\n",
    "\n",
    "plt.title('Smoothed MAE Curve - Cross-Validation Performance per Epoch')\n",
    "plt.xlabel('Epochs / CV Windows')\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.ylim(0, epoch_mae['mae_smooth'].max() + 2)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mae_crossval_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz6gpg7rPgj2"
   },
   "source": [
    "# **Personalized Fitness Recommender System**\n",
    "\n",
    "Sulla base delle predizioni fornite dal modello Informer per l'activity_index del giorno successivo, è stato integrato un sistema di raccomandazione personalizzato per ciascun utente, con l'obiettivo di migliorare il benessere attraverso suggerimenti mirati su sonno e attività fisica, come ad esempio:\n",
    "\n",
    "- Dormi più a lungo\n",
    "- Fai più attività fisica\n",
    "- Riduci i risvegli notturni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPIn2w8SAbTW"
   },
   "outputs": [],
   "source": [
    "def detailed_fitness_recommendation(row):\n",
    "    # Restituisce una raccomandazione personalizzata basata sull'activity_index e i fattori deboli.\n",
    "    index = row['activity_index']\n",
    "    feedback = []\n",
    "\n",
    "    if pd.isna(row['sleepduration']) or row['sleepduration'] < 6:\n",
    "        feedback.append(\"dormi di pi\\u00f9\")\n",
    "    if pd.isna(row['activity_duration']) or row['activity_duration'] / 60 < 30:\n",
    "        feedback.append(\"fai pi\\u00f9 attivit\\u00e0 fisica\")\n",
    "    if pd.isna(row['awakeduration']) or row['awakeduration'] > 1:\n",
    "        feedback.append(\"riduci i risvegli notturni\")\n",
    "    if pd.isna(row['deepduration']) or row['deepduration'] < 1:\n",
    "        feedback.append(\"aumenta il sonno profondo\")\n",
    "    if pd.isna(row['remduration']) or not (1 <= row['remduration'] <= 2):\n",
    "        feedback.append(\"ottimizza la fase REM\")\n",
    "    if pd.isna(row['stepsgaitspeed']) or row['stepsgaitspeed'] <= 100:\n",
    "        feedback.append(\"cammina a passo pi\\u00f9 sostenuto\")\n",
    "    if pd.isna(row['distancegaitspeed']) or row['distancegaitspeed'] <= 2:\n",
    "        feedback.append(\"aumenta la distanza percorsa\")\n",
    "    if pd.isna(row['activity_calories']) or row['activity_calories'] <= 100:\n",
    "        feedback.append(\"brucia pi\\u00f9 calorie\")\n",
    "\n",
    "    # Messaggio base in base all'index\n",
    "    if index >= 8:\n",
    "        base_msg = \"Ottimo lavoro! Continua cos\\u00ec.\"\n",
    "    elif 6 <= index < 8:\n",
    "        base_msg = \"Buon livello, ma c'\\u00e8 margine di miglioramento.\"\n",
    "    elif 4 <= index < 6:\n",
    "        base_msg = \"Livello moderato. Attenzione:\"\n",
    "    else:\n",
    "        base_msg = \"Livello basso. Consigliato intervenire su:\"\n",
    "\n",
    "    if feedback:\n",
    "        return f\"{base_msg} {', '.join(feedback[:3])}.\"\n",
    "    else:\n",
    "        return base_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHMKdZXrDAXF"
   },
   "source": [
    "Per generare raccomandazioni personalizzate, le previsioni dell'activity index sono state arricchite con le feature originali del dataset tramite un'operazione di merge sui campi user_id e data. Questo ha permesso di associare a ciascuna previsione informazioni reali come la durata del sonno o le calorie bruciate, fondamentali per applicare una funzione di raccomandazione dettagliata e fornire suggerimenti specifici per il benessere dell’utente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lpV-hfYAiaY"
   },
   "outputs": [],
   "source": [
    "# Unione forecast + dati reali per avere accesso alle feature necessarie\n",
    "forecast_with_features = pd.merge(\n",
    "    forecast,\n",
    "    dataset.reset_index(),\n",
    "    left_on=['unique_id', 'ds'],\n",
    "    right_on=['user_id', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Applicazione della funzione di raccomandazione dettagliata\n",
    "forecast_with_features['recommendation'] = forecast_with_features.apply(detailed_fitness_recommendation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXrzlhhzAohA"
   },
   "outputs": [],
   "source": [
    "#Visualizzazione di alcune raccomandazioni\n",
    "import random\n",
    "sample_users = random.sample(list(forecast_with_features['unique_id'].unique()), 5)\n",
    "\n",
    "for user in sample_users:\n",
    "    user_data = forecast_with_features[forecast_with_features['unique_id'] == user].sort_values('ds').tail(1)\n",
    "    print(f\"\\nUtente: {user}\")\n",
    "    print(f\"Data previsione: {user_data['ds'].dt.strftime('%Y-%m-%d').values[0]}\")\n",
    "    print(f\"Activity Index previsto: {user_data['Informer'].values[0]:.2f}\")\n",
    "    print(f\"Raccomandazione: {user_data['recommendation'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Jifev4i96OJ"
   },
   "source": [
    "# **Creazione dei modelli per confronto con Informer e Valutazione**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ckBS6FQzw_w"
   },
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def run_model(model_class, model_name, target_col='activity_index'):\n",
    "    print(f\"\\nAvvio addestramento del modello {model_name}...\")\n",
    "\n",
    "    # Inizializzazione del modello\n",
    "    model = model_class(\n",
    "        h=1,\n",
    "        input_size=target_sequence_len * 3,\n",
    "        learning_rate=1e-3,\n",
    "        max_steps=500,\n",
    "        val_check_steps=50,\n",
    "        windows_batch_size=32,\n",
    "        batch_size=32,\n",
    "        loss=DistributionLoss(distribution='StudentT', level=[80, 95]),\n",
    "        early_stop_patience_steps=150,\n",
    "        num_lr_decays=2,\n",
    "        scaler_type='standard'\n",
    "    )\n",
    "\n",
    "    nf = NeuralForecast(models=[model], freq='D')\n",
    "    nf.fit(df=train_df, val_size=1)\n",
    "\n",
    "    forecast = nf.predict(df=test_df)\n",
    "    print(\"Previsione completata!\")\n",
    "\n",
    "    # Conversioni e allineamento date/user\n",
    "    forecast['unique_id'] = forecast['unique_id'].astype(str)\n",
    "    forecast['ds'] = pd.to_datetime(forecast['ds'])\n",
    "\n",
    "    dataset_copy = dataset.reset_index()\n",
    "    dataset_copy['user_id'] = dataset_copy['user_id'].astype(str)\n",
    "    dataset_copy['date'] = pd.to_datetime(dataset_copy['date'])\n",
    "    dataset_copy = dataset_copy.set_index(['user_id', 'date'])\n",
    "\n",
    "    # Recupero dei target reali\n",
    "    true_targets = []\n",
    "    for idx, row in forecast.iterrows():\n",
    "        try:\n",
    "            y_real = dataset_copy.loc[(row['unique_id'], row['ds']), target_col]\n",
    "            y_real = y_real.iloc[0] if isinstance(y_real, pd.Series) else y_real\n",
    "        except KeyError:\n",
    "            y_real = np.nan\n",
    "        true_targets.append({'unique_id': row['unique_id'], 'ds': row['ds'], 'y_real': y_real})\n",
    "\n",
    "    real_targets_df = pd.DataFrame(true_targets)\n",
    "\n",
    "    # Merge tra predizione e valore reale\n",
    "    merged = pd.merge(forecast, real_targets_df, on=['unique_id', 'ds'], how='inner')\n",
    "    merged = merged.dropna(subset=['y_real', model_name])\n",
    "    print(f\"\\nRighe valide per il calcolo delle metriche: {merged.shape[0]}\")\n",
    "\n",
    "    # Metriche\n",
    "    if merged.empty:\n",
    "        print(\"Nessuna riga valida per il calcolo.\")\n",
    "    else:\n",
    "        mse = mean_squared_error(merged['y_real'], merged[model_name])\n",
    "        mae = mean_absolute_error(merged['y_real'], merged[model_name])\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(merged['y_real'], merged[model_name])\n",
    "\n",
    "        print(f\"\\n RISULTATI MODELLO {model_name}\")\n",
    "        print(f\"MSE:  {mse:.3f}\")\n",
    "        print(f\"MAE:  {mae:.3f}\")\n",
    "        print(f\"RMSE: {rmse:.3f}\")\n",
    "        print(f\"R²:   {r2:.3f}\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W74bVp7R928e"
   },
   "source": [
    "# **modello PatchTST**\n",
    "Il PatchTST è un modello di deep learning di ultima generazione, progettato specificamente per il forecasting di serie temporali multivariate. A differenza dei modelli tradizionali, PatchTST suddivide le sequenze temporali in piccoli segmenti (patch), facilitando l’apprendimento di pattern locali e globali. Questa struttura lo rende particolarmente efficace su dataset complessi come quello utilizzato in questo progetto, dove sono presenti più variabili correlate (sonno, attività fisica, indice di attività). PatchTST sfrutta i punti di forza dei Transformer, migliorando la gestione di lunghe sequenze e riducendo il rischio di overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrR9JoeOz-rt"
   },
   "outputs": [],
   "source": [
    "from neuralforecast.models import PatchTST\n",
    "merged_patchtst = run_model(PatchTST, 'PatchTST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_6lhyvs-EVx"
   },
   "source": [
    "# **modello Autoformer**\n",
    "Autoformer è un modello avanzato basato su Transformer, progettato per il forecasting di serie temporali a lungo termine. La sua architettura introduce un meccanismo di autocorrelazione che consente di catturare le dipendenze periodiche nei dati in modo più efficiente. A differenza dei Transformer tradizionali, Autoformer elimina la necessità di posizioni assolute, concentrandosi invece sulla ripetitività intrinseca delle serie temporali. Questo approccio lo rende particolarmente adatto a scenari in cui i pattern ciclici, come quelli legati al sonno e all’attività fisica, giocano un ruolo centrale nella previsione dell’indice di benessere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBT4UCK01OM8"
   },
   "outputs": [],
   "source": [
    "from neuralforecast.models import Autoformer\n",
    "merged_autoformer = run_model(Autoformer, 'Autoformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUCxT_55-KUi"
   },
   "source": [
    "# **modello NHITS**\n",
    "NHITS (Neural Hierarchical Interpolation for Time Series) è un modello MLP-based che adotta un approccio gerarchico per la ricostruzione delle serie temporali. Utilizza una struttura a blocchi progressivi per approssimare il segnale target attraverso raffinamenti successivi, riducendo l’errore di previsione a ogni passaggio. NHITS si distingue per l'efficienza computazionale e la robustezza alle fluttuazioni dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuXM8sUm3Tot"
   },
   "outputs": [],
   "source": [
    "from neuralforecast.models import NHITS\n",
    "merged_nhits = run_model(NHITS, 'NHITS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl6AlnXI-N-M"
   },
   "source": [
    "# **modello TimesNet**\n",
    "TimesNet è un modello basato su convoluzioni gerarchiche pensato per il forecasting di serie temporali univariate e multivariate. Introduce un nuovo paradigma che sfrutta kernel convoluzionali a diversi livelli per catturare dinamiche temporali a scala multipla. Grazie alla sua capacità di apprendere rappresentazioni temporali ricche ed efficienti, TimesNet si adatta bene a serie complesse e rumorose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlyBDQYw34yH"
   },
   "outputs": [],
   "source": [
    "from neuralforecast.models import TimesNet\n",
    "merged_nhits = run_model(TimesNet, 'TimesNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dt27Ij79V2G"
   },
   "source": [
    "# **Conclusione**\n",
    "\n",
    "\n",
    "Dal confronto finale tra i modelli testati emerge che Informer rappresenta la soluzione più efficace per la previsione dell’activity_index, grazie a prestazioni complessive superiori su tutte le metriche considerate. In particolare, Informer ottiene il MSE più basso (0.923), il MAE più contenuto (0.702) e il valore più alto di R² (0.555), evidenziando una maggiore precisione e una migliore capacità di spiegare la variabilità del target rispetto agli altri modelli.\n",
    "\n",
    "A confronto, PatchTST mostra performance solide (MSE = 1.341, MAE = 0.880, R² = 0.354), ma meno efficaci nel catturare le dinamiche temporali complesse del dataset. Anche modelli più recenti come TimesNet (MSE = 1.906, MAE = 0.989, R² = 0.082) e Autoformer (MSE = 1.983, MAE = 1.006) non riescono a raggiungere gli stessi livelli di accuratezza. Infine, NHITS risulta il meno performante tra quelli testati, con un MSE di 2.739 e un MAE di 1.153.\n",
    "\n",
    "Nel complesso, l’architettura di Informer si dimostra particolarmente adatta al contesto multivariato e alle forti dipendenze temporali del problema analizzato, confermandosi come il modello più robusto e affidabile tra quelli sperimentati."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNYmPwuNfN0xLn1hgxVcfGy",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
